{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulianCarax01/Fondamenti-web-app/blob/main/DijkstraSerialCPPMerge_prova_raff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --mode launch-and-attach -o profile --target-processes all --nvtx --call-stack --section ComputeWorkloadAnalysis --section MemoryWorkloadAnalysis -f ./dijkstra_serialMerge.o\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bx_4VKs4zwdy",
        "outputId": "94f1d091-a602-49c3-c5bd-f93c1b69fff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==ERROR== './dijkstra_serialMerge.o' does not exist or is not an executable. Please make sure to specify the absolute path to './dijkstra_serialMerge.o' if the executable is not in the local directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEenND-3v_3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dijkstra_serialMerge_3kern.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <climits>\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <chrono>\n",
        "\n",
        "// Funzione per eseguire Dijkstra in modo seriale sulla CPU\n",
        "void dijkstraSerial(const int *graph, int n, int start, int *distances) {\n",
        "    // Array per tenere traccia dei nodi visitati\n",
        "    std::vector<int> visited(n, 0);\n",
        "\n",
        "    // Inizializza le distanze\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        distances[i] = (i == start) ? 0 : INT_MAX;\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        int minDist = INT_MAX;\n",
        "        int currentNode = -1;\n",
        "\n",
        "        // Trova il nodo con la distanza minima non ancora visitato\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            if (!visited[j] && distances[j] < minDist) {\n",
        "                minDist = distances[j];\n",
        "                currentNode = j;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if (currentNode == -1) break; // Nessun altro nodo raggiungibile\n",
        "\n",
        "        // Marca il nodo corrente come visitato\n",
        "        visited[currentNode] = 1;\n",
        "\n",
        "        // Aggiorna le distanze per i nodi adiacenti\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            int weight = graph[currentNode * n + j]; // Peso dell'arco currentNode -> j\n",
        "            if (weight > 0 && weight != INT_MAX) {  // Esiste un arco valido\n",
        "                // Verifica per evitare overflow: dist[currentNode] + weight\n",
        "                if (distances[currentNode] != INT_MAX &&\n",
        "                    distances[currentNode] + weight < distances[j]) {\n",
        "                    distances[j] = distances[currentNode] + weight;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel CUDA per aggiornare le distanze\n",
        "__global__ void cudaNodeRelax(int nodo, int *graph, int *distances, int *visited, int n) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Carica la riga del nodo corrente nella memoria condivisa\n",
        "    extern __shared__ int sharedRow[];\n",
        "    if (tid < n) {\n",
        "        sharedRow[tid] = graph[nodo * n + tid];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    if (tid < n && visited[tid] == 0) { // Assicurati che il nodo non sia stato visitato\n",
        "        int weight = sharedRow[tid]; // Peso dell'arco (nodo -> tid)\n",
        "        if (weight != INT_MAX && weight != 0) { // Arco esistente e diverso da se stesso\n",
        "            atomicMin(&distances[tid], distances[nodo] + weight); // Aggiorna la distanza con atomicMin\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void findMinReductionCUDA(int *distanze, int *visitato, int n, int *minNodo, int *minDistanza) {\n",
        "    // Memoria condivisa dinamica\n",
        "    extern __shared__ int sharedMem[];\n",
        "\n",
        "    // Dividi la memoria condivisa in due array distinti\n",
        "    int *sharedDistanze = sharedMem;                           // Primo array: memorizza le distanze\n",
        "    int *sharedNodi = &sharedDistanze[n];                     // Secondo array: memorizza gli indici dei nodi\n",
        "\n",
        "    int tid = threadIdx.x;                   // Indice del thread locale al blocco\n",
        "    int gid = blockIdx.x * blockDim.x + tid; // Indice globale del thread\n",
        "\n",
        "    // Inizializzazione della memoria condivisa\n",
        "    if (gid < n && tid < blockDim.x) {\n",
        "        sharedDistanze[tid] = (visitato[gid] == 0) ? distanze[gid] : INT_MAX;\n",
        "        sharedNodi[tid] = gid;\n",
        "    } else {\n",
        "        sharedDistanze[tid] = INT_MAX;\n",
        "        sharedNodi[tid] = -1;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Riduzione parallela per trovare il minimo\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        if (gid < n && tid + stride < blockDim.x) {\n",
        "            if (sharedDistanze[tid + stride] < sharedDistanze[tid]) {\n",
        "                sharedDistanze[tid] = sharedDistanze[tid + stride];\n",
        "                sharedNodi[tid] = sharedNodi[tid + stride];\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Il thread 0 del blocco salva il risultato parziale in modo atomico\n",
        "    if (tid == 0) {\n",
        "        atomicExch(minNodo, sharedNodi[0]);\n",
        "        atomicExch(minDistanza, sharedDistanze[0]);\n",
        "        atomicExch(&visitato[sharedNodi[0]], 1); // Segna come visitato\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void initialValueKernel(int*distances, int*visited,int start,int n){\n",
        "  int id = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (id < n) {\n",
        "        distances[id] = (id == start) ? 0 : INFINITY;\n",
        "        visited[id] = 0;\n",
        "    }\n",
        "}\n",
        "\n",
        "void dijkstraCUDA(int *graph, int n, int start, int *distances)  {\n",
        "    // Allocazione memoria su GPU\n",
        "    int *d_graph,*d_minNodo, *d_minDistanza, *d_distances, *d_visited;\n",
        "\n",
        "    // Allocazione memoria sulla GPU\n",
        "    cudaMalloc((void **)&d_graph, n * n * sizeof(int));\n",
        "    cudaMalloc((void **)&d_distances, n * sizeof(int));\n",
        "    cudaMalloc((void **)&d_visited, n * sizeof(int));\n",
        "    cudaMalloc(&d_minNodo, sizeof(int));\n",
        "    cudaMalloc(&d_minDistanza, sizeof(int));\n",
        "\n",
        "    int blockSize = n;\n",
        "    int gridDim=(n + blockSize - 1) / blockSize;\n",
        "    initialValueKernel<<<gridDim,blockSize>>>(d_distances,d_visited,start,n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copia il grafo e i dati iniziali sulla GPU\n",
        "    cudaMemcpy(d_graph, graph, n * n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        // Resetta `minDistanza` al massimo valore possibile\n",
        "        int h_minDistanza = INT_MAX;\n",
        "        cudaMemcpy(d_minDistanza, &h_minDistanza, sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Trova il nodo con la distanza minima non visitato\n",
        "        blockSize = n;\n",
        "        gridDim=(n + blockSize - 1) / blockSize;\n",
        "        findMinReductionCUDA<<<gridDim, blockSize, blockSize * sizeof(int) * 2>>>(d_distances, d_visited, n, d_minNodo, d_minDistanza);\n",
        "\n",
        "        if (cudaDeviceSynchronize() != cudaSuccess) {\n",
        "        std::cerr << \"Errore durante l'esecuzione del kernel.\" << std::endl;\n",
        "        return;\n",
        "        }\n",
        "\n",
        "        // Recupera il nodo con distanza minima dalla GPU\n",
        "        int h_minNodo;\n",
        "        cudaMemcpy(&h_minNodo, d_minNodo, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "        cudaMemcpy(&h_minDistanza, d_minDistanza, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "        // Se non ci sono piÃ¹ nodi raggiungibili, esci\n",
        "        if (h_minNodo == -1 || h_minDistanza == INT_MAX) {\n",
        "            break;\n",
        "        }\n",
        "        cudaNodeRelax<<<gridDim, blockSize, blockSize * sizeof(int)>>>(h_minNodo, d_graph, d_distances, d_visited, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    // Copia i risultati dalla GPU alla CPU\n",
        "    cudaMemcpy(distances, d_distances, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Libera memoria GPU\n",
        "    cudaFree(d_graph);\n",
        "    cudaFree(d_distances);\n",
        "    cudaFree(d_visited);\n",
        "    cudaFree(d_minNodo);\n",
        "    cudaFree(d_minDistanza);\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__global__ void generateGraphKernel(int *graph, int n, int minWeight, int maxWeight, unsigned int seed) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Stato del generatore casuale\n",
        "    curandState state;\n",
        "\n",
        "    // Grid-stride loop per coprire tutta la matrice\n",
        "    for (int i = idx; i < n * n; i += blockDim.x * gridDim.x) {\n",
        "        int row = i / n; // Calcola la riga\n",
        "        int col = i % n; // Calcola la colonna\n",
        "\n",
        "        // Inizializza lo stato casuale\n",
        "        curand_init(seed + i, 0, 0, &state);\n",
        "\n",
        "        // Calcola il peso per la triangolare superiore\n",
        "        int weight = (row < col)\n",
        "                     ? ((curand(&state) % 100 < 40) ? curand(&state) % (maxWeight - minWeight + 1) + minWeight : INT_MAX)\n",
        "                     : 0;\n",
        "\n",
        "        // Scrive nella memoria globale\n",
        "        if (row < col) {\n",
        "            graph[row * n + col] = weight;\n",
        "            graph[col * n + row] = weight; // Simmetria\n",
        "        }\n",
        "\n",
        "        // Imposta la diagonale a 0\n",
        "        if (row == col) {\n",
        "            graph[row * n + col] = 0;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "void generateGraphCUDA(int *graph, int n, int minWeight, int maxWeight) {\n",
        "    int *d_graph;\n",
        "    size_t size = n * n * sizeof(int);\n",
        "\n",
        "    // Allocazione della memoria sulla GPU\n",
        "    if (cudaMalloc(&d_graph, size) != cudaSuccess) {\n",
        "        std::cerr << \"Errore nell'allocazione della memoria sulla GPU.\" << std::endl;\n",
        "        return;\n",
        "    }\n",
        "    if (cudaMemset(d_graph, 0, size) != cudaSuccess) {\n",
        "        std::cerr << \"Errore durante l'inizializzazione della memoria sulla GPU.\" << std::endl;\n",
        "        cudaFree(d_graph);\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Configura blocchi e griglia con valori fissi\n",
        "    dim3 threadsPerBlock(1024, 1, 1);\n",
        "    dim3 blocksPerGrid((n + threadsPerBlock.x - 1) / threadsPerBlock.x, 1, 1);\n",
        "\n",
        "\n",
        "    // Lancia il kernel con configurazione fissa\n",
        "    generateGraphKernel<<<blocksPerGrid, threadsPerBlock>>>(d_graph, n, minWeight, maxWeight, time(NULL));\n",
        "\n",
        "    if (cudaDeviceSynchronize() != cudaSuccess) {\n",
        "        std::cerr << \"Errore durante l'esecuzione del kernel.\" << std::endl;\n",
        "        cudaFree(d_graph);\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Copia i dati dalla GPU alla CPU\n",
        "    if (cudaMemcpy(graph, d_graph, size, cudaMemcpyDeviceToHost) != cudaSuccess) {\n",
        "        std::cerr << \"Errore nella copia dei dati dalla GPU alla CPU.\" << std::endl;\n",
        "        cudaFree(d_graph);\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Libera memoria GPU\n",
        "    cudaFree(d_graph);\n",
        "}\n",
        "\n",
        "\n",
        "void generateGraphSerial(int *graph, int n, int minWeight, int maxWeight) {\n",
        "    // Inizializza il generatore di numeri casuali\n",
        "    std::srand(std::time(0));\n",
        "\n",
        "    for (int row = 0; row < n; ++row) {\n",
        "        for (int col = 0; col < n; ++col) {\n",
        "            if (row == col) {\n",
        "                // Imposta la diagonale principale a 0\n",
        "                graph[row * n + col] = 0;\n",
        "            } else if (row < col) {\n",
        "                // Genera un arco casuale per la triangolare superiore\n",
        "                int randomValue = std::rand() % 100;\n",
        "                if (randomValue < 40) { // 40% probabilitÃ  di avere un arco\n",
        "                    int weight = std::rand() % (maxWeight - minWeight + 1) + minWeight;\n",
        "                    graph[row * n + col] = weight;\n",
        "                    graph[col * n + row] = weight; // Simmetria\n",
        "                } else {\n",
        "                    graph[row * n + col] = INT_MAX; // Nessun arco\n",
        "                    graph[col * n + row] = INT_MAX; // Nessun arco\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    int n = 32768; // Numero di nodi\n",
        "    int start = 0; // Nodo iniziale\n",
        "    int minWeight = 1;\n",
        "    int maxWeight = 2000;\n",
        "\n",
        "    std::vector<int> graph(n * n);\n",
        "    std::vector<int> graphSerial(n * n);\n",
        "\n",
        "    // Generazione del grafo Seriale\n",
        "auto startTimeS = std::chrono::high_resolution_clock::now();\n",
        "generateGraphSerial(graphSerial.data(), n, minWeight, maxWeight);\n",
        "auto endTimeS = std::chrono::high_resolution_clock::now();\n",
        "long serialGraphTime = std::chrono::duration_cast<std::chrono::microseconds>(endTimeS - startTimeS).count();\n",
        "\n",
        "\n",
        "// Generazione del grafo con CUDA\n",
        "auto startTime = std::chrono::high_resolution_clock::now();\n",
        "generateGraphCUDA(graph.data(), n, minWeight, maxWeight);\n",
        "auto endTime = std::chrono::high_resolution_clock::now();\n",
        "long cudaGraphTime = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();\n",
        "\n",
        "\n",
        "// Calcolo delle distanze con Dijkstra Seriale\n",
        "std::vector<int> serialDistances(n);\n",
        "startTime = std::chrono::high_resolution_clock::now();\n",
        "dijkstraSerial(graph.data(), n, start, serialDistances.data());\n",
        "endTime = std::chrono::high_resolution_clock::now();\n",
        "long serialTime = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();\n",
        "\n",
        "// Calcolo delle distanze con Dijkstra CUDA\n",
        "std::vector<int> cudaDistances(n);\n",
        "startTime = std::chrono::high_resolution_clock::now();\n",
        "dijkstraCUDA(graph.data(), n, start, cudaDistances.data());\n",
        "endTime = std::chrono::high_resolution_clock::now();\n",
        "long cudaTime = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();\n",
        "\n",
        "/*\n",
        "// Stampa del grafo\n",
        "std::cout << \"=== Matrice del grafo ===\\n\";\n",
        "for (int i = 0; i < n; ++i) {\n",
        "    for (int j = 0; j < n; ++j) {\n",
        "        if (i == j) {\n",
        "            std::cout << \"X \"; // Diagonale principale\n",
        "        } else if (graph[i * n + j] == INT_MAX) {\n",
        "            std::cout << \"_ \"; // Nessun collegamento\n",
        "        } else {\n",
        "            std::cout << graph[i * n + j] << \" \"; // Peso dell'arco\n",
        "        }\n",
        "    }\n",
        "    std::cout << \"\\n\";\n",
        "}\n",
        "\n",
        "// Confronto distanze Seriali vs CUDA\n",
        "std::cout << \"\\n=== Distanze (Seriale vs CUDA) ===\\n\";\n",
        "std::cout << \"Nodo\\tSeriale\\tCUDA\\n\";\n",
        "for (int i = 0; i < n; ++i) {\n",
        "    std::cout << i << \"\\t\";\n",
        "    if (serialDistances[i] == INT_MAX)\n",
        "        std::cout << \"â\\t\";\n",
        "    else\n",
        "        std::cout << serialDistances[i] << \"\\t\";\n",
        "\n",
        "    if (cudaDistances[i] == INT_MAX)\n",
        "        std::cout << \"â\\n\";\n",
        "    else\n",
        "        std::cout << cudaDistances[i] << \"\\n\";\n",
        "}\n",
        "*/\n",
        "\n",
        "// Stampa dei tempi di generazione e di esecuzione\n",
        "std::cout << \"\\n=== Tempi di generazione ===\\n\";\n",
        "std::cout << \"Tempo di generazione Seriale: \" << serialGraphTime << \" Âµs\\n\";\n",
        "std::cout << \"Tempo di generazione CUDA: \" << cudaGraphTime << \" Âµs\\n\";\n",
        "\n",
        "std::cout << \"\\n=== Tempi di esecuzione Dijkstra ===\\n\";\n",
        "std::cout << \"Tempo esecuzione Seriale: \" << serialTime << \" Âµs\\n\";\n",
        "std::cout << \"Tempo esecuzione CUDA: \" << cudaTime << \" Âµs\\n\";\n",
        "\n",
        "return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwFhMJZKeA3S",
        "outputId": "c32e780b-3251-4ee7-931c-1aa136588289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dijkstra_serialMerge_3kern.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -rdc=true dijkstra_serialMerge_3kern.cu -lcudadevrt -o dijkstra_serialMerge_3kern.o\n"
      ],
      "metadata": {
        "id": "zZBx0b-oyUF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./dijkstra_serialMerge_3kern.o"
      ],
      "metadata": {
        "id": "REBDAqA6GRNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --mode launch-and-attach -o profile --target-processes all --nvtx --call-stack --section ComputeWorkloadAnalysis --section MemoryWorkloadAnalysis -f ./dijkstra_serialMerge_3kern.o"
      ],
      "metadata": {
        "id": "lDctyfDLZwMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --target-processes=all ./dijkstra_serialMerge_3kern.o"
      ],
      "metadata": {
        "id": "fC33kGZlVqCl",
        "outputId": "6bd825ce-ca7a-47c3-aff8-3f1b97836238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 14359 (/content/dijkstra_serialMerge_3kern.o)\n",
            "==PROF== Profiling \"generateGraphKernel\" - 0: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"initialValueKernel\" - 1: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 2: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 3: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 4: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 5: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 6: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 7: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 8: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 9: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 10: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 11: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 12: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 13: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 14: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 15: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 16: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 17: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 18: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 19: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 20: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 21: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 22: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 23: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 24: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 25: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 26: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 27: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 28: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 29: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 30: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 31: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 32: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 33: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 34: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 35: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 36: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 37: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 38: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 39: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 40: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 41: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 42: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 43: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 44: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 45: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 46: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 47: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 48: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 49: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 50: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 51: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 52: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 53: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 54: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 55: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 56: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 57: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 58: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 59: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 60: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 61: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 62: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 63: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 64: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 65: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 66: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 67: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 68: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 69: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 70: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 71: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"findMinReductionCUDA\" - 72: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"cudaNodeRelax\" - 73: 0%...==PROF== Received signal\n",
            "==PROF== Trying to shutdown target application\n",
            " - 1 pass\n",
            "==ERROR== Failed to profile \"cudaNodeRelax\" in process 14359\n",
            "==PROF== Trying to shutdown target application\n",
            "==ERROR== An error occurred while trying to profile.\n",
            "==WARNING== Found outstanding GPU clock reset, trying to revert...Success.\n",
            "[14359] dijkstra_serialMerge_3kern.o@127.0.0.1\n",
            "  generateGraphKernel(int *, int, int, int, unsigned int) (256, 4, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.92\n",
            "    SM Frequency            cycle/usecond       576.09\n",
            "    Elapsed Cycles                  cycle      288,972\n",
            "    Memory Throughput                   %        39.27\n",
            "    DRAM Throughput                     %        16.44\n",
            "    Duration                      usecond       501.60\n",
            "    L1/TEX Cache Throughput             %        78.54\n",
            "    L2 Cache Throughput                 %        33.84\n",
            "    SM Active Cycles                cycle   279,798.03\n",
            "    Compute (SM) Throughput             %        21.96\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n",
            "          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n",
            "          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,024\n",
            "    Registers Per Thread             register/thread              20\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread       1,048,576\n",
            "    Waves Per SM                                               25.60\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            2\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        98.40\n",
            "    Achieved Active Warps Per SM           warp        31.49\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  initialValueKernel(int *, int *, int, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.89\n",
            "    SM Frequency            cycle/usecond       570.08\n",
            "    Elapsed Cycles                  cycle        1,971\n",
            "    Memory Throughput                   %         0.66\n",
            "    DRAM Throughput                     %         0.05\n",
            "    Duration                      usecond         3.46\n",
            "    L1/TEX Cache Throughput             %        54.24\n",
            "    L2 Cache Throughput                 %         0.66\n",
            "    SM Active Cycles                cycle        19.18\n",
            "    Compute (SM) Throughput             %         0.24\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        88.25\n",
            "    Achieved Active Warps Per SM           warp        28.24\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 11.75%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (88.2%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.96\n",
            "    SM Frequency            cycle/usecond       581.12\n",
            "    Elapsed Cycles                  cycle        7,067\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.25\n",
            "    Duration                      usecond        12.16\n",
            "    L1/TEX Cache Throughput             %        54.93\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       146.55\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.09\n",
            "    Achieved Active Warps Per SM           warp        29.79\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.82\n",
            "    SM Frequency            cycle/usecond       560.96\n",
            "    Elapsed Cycles                  cycle        3,070\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.73\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.20\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.67\n",
            "    Achieved Active Warps Per SM           warp        30.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.88\n",
            "    SM Frequency            cycle/usecond       572.33\n",
            "    Elapsed Cycles                  cycle        6,997\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.25\n",
            "    Duration                      usecond        12.22\n",
            "    L1/TEX Cache Throughput             %        55.37\n",
            "    L2 Cache Throughput                 %         0.31\n",
            "    SM Active Cycles                cycle       145.38\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.23\n",
            "    Achieved Active Warps Per SM           warp        29.83\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.66\n",
            "    SM Frequency            cycle/usecond       543.25\n",
            "    Elapsed Cycles                  cycle        3,060\n",
            "    Memory Throughput                   %         0.96\n",
            "    DRAM Throughput                     %         0.96\n",
            "    Duration                      usecond         5.63\n",
            "    L1/TEX Cache Throughput             %        23.53\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.60\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.70\n",
            "    Achieved Active Warps Per SM           warp        30.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.88\n",
            "    SM Frequency            cycle/usecond       571.17\n",
            "    Elapsed Cycles                  cycle        6,946\n",
            "    Memory Throughput                   %         1.16\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.16\n",
            "    L1/TEX Cache Throughput             %        55.02\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       146.30\n",
            "    Compute (SM) Throughput             %         1.16\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.05\n",
            "    Achieved Active Warps Per SM           warp        29.78\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.77\n",
            "    SM Frequency            cycle/usecond       562.39\n",
            "    Elapsed Cycles                  cycle        3,078\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.44\n",
            "    L2 Cache Throughput                 %         0.60\n",
            "    SM Active Cycles                cycle        47.77\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.71\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.91\n",
            "    SM Frequency            cycle/usecond       573.49\n",
            "    Elapsed Cycles                  cycle        6,992\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.25\n",
            "    Duration                      usecond        12.19\n",
            "    L1/TEX Cache Throughput             %        54.99\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       146.40\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.02\n",
            "    Achieved Active Warps Per SM           warp        29.77\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.94\n",
            "    SM Frequency            cycle/usecond       574.71\n",
            "    Elapsed Cycles                  cycle        3,127\n",
            "    Memory Throughput                   %         0.84\n",
            "    DRAM Throughput                     %         0.84\n",
            "    Duration                      usecond         5.44\n",
            "    L1/TEX Cache Throughput             %        23.46\n",
            "    L2 Cache Throughput                 %         0.55\n",
            "    SM Active Cycles                cycle        47.75\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.71\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.87\n",
            "    SM Frequency            cycle/usecond       570.73\n",
            "    Elapsed Cycles                  cycle        6,959\n",
            "    Memory Throughput                   %         1.16\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.19\n",
            "    L1/TEX Cache Throughput             %        54.87\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       146.70\n",
            "    Compute (SM) Throughput             %         1.16\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.10\n",
            "    Achieved Active Warps Per SM           warp        29.79\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.80\n",
            "    SM Frequency            cycle/usecond       560.29\n",
            "    Elapsed Cycles                  cycle        3,049\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.44\n",
            "    L1/TEX Cache Throughput             %        23.72\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.23\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.69\n",
            "    Achieved Active Warps Per SM           warp        30.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.95\n",
            "    SM Frequency            cycle/usecond       581.05\n",
            "    Elapsed Cycles                  cycle        7,029\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.10\n",
            "    L1/TEX Cache Throughput             %        54.82\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       146.85\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.09\n",
            "    Achieved Active Warps Per SM           warp        29.79\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.77\n",
            "    SM Frequency            cycle/usecond       561.51\n",
            "    Elapsed Cycles                  cycle        3,074\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.46\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.75\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.72\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.94\n",
            "    SM Frequency            cycle/usecond       578.37\n",
            "    Elapsed Cycles                  cycle        7,034\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.16\n",
            "    L1/TEX Cache Throughput             %        54.82\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       146.85\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.09\n",
            "    Achieved Active Warps Per SM           warp        29.79\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.85\n",
            "    SM Frequency            cycle/usecond       566.76\n",
            "    Elapsed Cycles                  cycle        3,084\n",
            "    Memory Throughput                   %         0.86\n",
            "    DRAM Throughput                     %         0.86\n",
            "    Duration                      usecond         5.44\n",
            "    L1/TEX Cache Throughput             %        23.36\n",
            "    L2 Cache Throughput                 %         0.63\n",
            "    SM Active Cycles                cycle        47.95\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.68\n",
            "    Achieved Active Warps Per SM           warp        30.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.97\n",
            "    SM Frequency            cycle/usecond       579.39\n",
            "    Elapsed Cycles                  cycle        6,990\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.25\n",
            "    Duration                      usecond        12.06\n",
            "    L1/TEX Cache Throughput             %        54.65\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       147.30\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.11\n",
            "    Achieved Active Warps Per SM           warp        29.80\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.86\n",
            "    SM Frequency            cycle/usecond       567.88\n",
            "    Elapsed Cycles                  cycle        3,126\n",
            "    Memory Throughput                   %         1.00\n",
            "    DRAM Throughput                     %         1.00\n",
            "    Duration                      usecond         5.50\n",
            "    L1/TEX Cache Throughput             %        23.58\n",
            "    L2 Cache Throughput                 %         0.56\n",
            "    SM Active Cycles                cycle        47.50\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.71\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.89\n",
            "    SM Frequency            cycle/usecond       574.69\n",
            "    Elapsed Cycles                  cycle        6,989\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.27\n",
            "    Duration                      usecond        12.16\n",
            "    L1/TEX Cache Throughput             %        54.55\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       147.57\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.07\n",
            "    Achieved Active Warps Per SM           warp        29.78\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.79\n",
            "    SM Frequency            cycle/usecond       563.19\n",
            "    Elapsed Cycles                  cycle        3,101\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.50\n",
            "    L1/TEX Cache Throughput             %        23.42\n",
            "    L2 Cache Throughput                 %         0.56\n",
            "    SM Active Cycles                cycle        47.83\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.73\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.90\n",
            "    SM Frequency            cycle/usecond       571.25\n",
            "    Elapsed Cycles                  cycle        6,983\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.22\n",
            "    L1/TEX Cache Throughput             %        55.01\n",
            "    L2 Cache Throughput                 %         0.31\n",
            "    SM Active Cycles                cycle       146.32\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.05\n",
            "    Achieved Active Warps Per SM           warp        29.78\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.80\n",
            "    SM Frequency            cycle/usecond       559.87\n",
            "    Elapsed Cycles                  cycle        3,065\n",
            "    Memory Throughput                   %         0.90\n",
            "    DRAM Throughput                     %         0.90\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.72\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.23\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.68\n",
            "    Achieved Active Warps Per SM           warp        30.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.89\n",
            "    SM Frequency            cycle/usecond       570.95\n",
            "    Elapsed Cycles                  cycle        6,998\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.27\n",
            "    Duration                      usecond        12.26\n",
            "    L1/TEX Cache Throughput             %        55.09\n",
            "    L2 Cache Throughput                 %         0.31\n",
            "    SM Active Cycles                cycle       146.12\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.47\n",
            "    Achieved Active Warps Per SM           warp        29.91\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.80\n",
            "    SM Frequency            cycle/usecond       561.81\n",
            "    Elapsed Cycles                  cycle        3,075\n",
            "    Memory Throughput                   %         0.86\n",
            "    DRAM Throughput                     %         0.86\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.46\n",
            "    L2 Cache Throughput                 %         0.63\n",
            "    SM Active Cycles                cycle        47.75\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.72\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.94\n",
            "    SM Frequency            cycle/usecond       577.67\n",
            "    Elapsed Cycles                  cycle        7,062\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.22\n",
            "    L1/TEX Cache Throughput             %        55.05\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       146.22\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.06\n",
            "    Achieved Active Warps Per SM           warp        29.78\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.78\n",
            "    SM Frequency            cycle/usecond       559.56\n",
            "    Elapsed Cycles                  cycle        3,045\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.44\n",
            "    L1/TEX Cache Throughput             %        23.73\n",
            "    L2 Cache Throughput                 %         0.60\n",
            "    SM Active Cycles                cycle        47.20\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.64\n",
            "    Achieved Active Warps Per SM           warp        30.92\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.82\n",
            "    SM Frequency            cycle/usecond       566.31\n",
            "    Elapsed Cycles                  cycle        6,978\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.32\n",
            "    L1/TEX Cache Throughput             %        55.39\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       145.32\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.40\n",
            "    Achieved Active Warps Per SM           warp        29.89\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.77\n",
            "    SM Frequency            cycle/usecond       557.31\n",
            "    Elapsed Cycles                  cycle        3,051\n",
            "    Memory Throughput                   %         0.94\n",
            "    DRAM Throughput                     %         0.94\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.42\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.83\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.72\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.90\n",
            "    SM Frequency            cycle/usecond       573.40\n",
            "    Elapsed Cycles                  cycle        7,010\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.22\n",
            "    L1/TEX Cache Throughput             %        54.90\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       146.62\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.05\n",
            "    Achieved Active Warps Per SM           warp        29.78\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.83\n",
            "    SM Frequency            cycle/usecond       563.76\n",
            "    Elapsed Cycles                  cycle        3,049\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.41\n",
            "    L1/TEX Cache Throughput             %        23.59\n",
            "    L2 Cache Throughput                 %         0.60\n",
            "    SM Active Cycles                cycle        47.48\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.69\n",
            "    Achieved Active Warps Per SM           warp        30.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.88\n",
            "    SM Frequency            cycle/usecond       570.46\n",
            "    Elapsed Cycles                  cycle        6,956\n",
            "    Memory Throughput                   %         1.16\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.19\n",
            "    L1/TEX Cache Throughput             %        54.72\n",
            "    L2 Cache Throughput                 %         0.28\n",
            "    SM Active Cycles                cycle       147.10\n",
            "    Compute (SM) Throughput             %         1.16\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.05\n",
            "    Achieved Active Warps Per SM           warp        29.78\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.79\n",
            "    SM Frequency            cycle/usecond       558.21\n",
            "    Elapsed Cycles                  cycle        3,073\n",
            "    Memory Throughput                   %         0.88\n",
            "    DRAM Throughput                     %         0.88\n",
            "    Duration                      usecond         5.50\n",
            "    L1/TEX Cache Throughput             %        22.97\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        48.75\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.79\n",
            "    Achieved Active Warps Per SM           warp        30.97\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.90\n",
            "    SM Frequency            cycle/usecond       572.95\n",
            "    Elapsed Cycles                  cycle        7,042\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.29\n",
            "    L1/TEX Cache Throughput             %        54.34\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       148.15\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.10\n",
            "    Achieved Active Warps Per SM           warp        29.79\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.89\n",
            "    SM Frequency            cycle/usecond       571.24\n",
            "    Elapsed Cycles                  cycle        3,126\n",
            "    Memory Throughput                   %         0.84\n",
            "    DRAM Throughput                     %         0.84\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.47\n",
            "    L2 Cache Throughput                 %         0.55\n",
            "    SM Active Cycles                cycle        47.73\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.71\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.77\n",
            "    SM Frequency            cycle/usecond       560.04\n",
            "    Elapsed Cycles                  cycle        6,954\n",
            "    Memory Throughput                   %         1.16\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.42\n",
            "    L1/TEX Cache Throughput             %        55.09\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       146.12\n",
            "    Compute (SM) Throughput             %         1.16\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.33\n",
            "    Achieved Active Warps Per SM           warp        29.87\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.82\n",
            "    SM Frequency            cycle/usecond       562.50\n",
            "    Elapsed Cycles                  cycle        3,079\n",
            "    Memory Throughput                   %         0.86\n",
            "    DRAM Throughput                     %         0.86\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.57\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.52\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.59\n",
            "    Achieved Active Warps Per SM           warp        30.91\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.91\n",
            "    SM Frequency            cycle/usecond       574.07\n",
            "    Elapsed Cycles                  cycle        7,036\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.26\n",
            "    L1/TEX Cache Throughput             %        53.72\n",
            "    L2 Cache Throughput                 %         0.31\n",
            "    SM Active Cycles                cycle       149.85\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.51\n",
            "    Achieved Active Warps Per SM           warp        29.92\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.85\n",
            "    SM Frequency            cycle/usecond       570.40\n",
            "    Elapsed Cycles                  cycle        3,104\n",
            "    Memory Throughput                   %         0.86\n",
            "    DRAM Throughput                     %         0.86\n",
            "    Duration                      usecond         5.44\n",
            "    L1/TEX Cache Throughput             %        22.75\n",
            "    L2 Cache Throughput                 %         0.56\n",
            "    SM Active Cycles                cycle        49.23\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.82\n",
            "    Achieved Active Warps Per SM           warp        30.98\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.92\n",
            "    SM Frequency            cycle/usecond       577.96\n",
            "    Elapsed Cycles                  cycle        7,066\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.22\n",
            "    L1/TEX Cache Throughput             %        54.92\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       146.57\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        92.99\n",
            "    Achieved Active Warps Per SM           warp        29.76\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.80\n",
            "    SM Frequency            cycle/usecond       560.93\n",
            "    Elapsed Cycles                  cycle        3,070\n",
            "    Memory Throughput                   %         0.86\n",
            "    DRAM Throughput                     %         0.86\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.68\n",
            "    L2 Cache Throughput                 %         0.60\n",
            "    SM Active Cycles                cycle        47.30\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.69\n",
            "    Achieved Active Warps Per SM           warp        30.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.85\n",
            "    SM Frequency            cycle/usecond       568.10\n",
            "    Elapsed Cycles                  cycle        7,018\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.35\n",
            "    L1/TEX Cache Throughput             %        54.74\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       147.05\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.07\n",
            "    Achieved Active Warps Per SM           warp        29.78\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.74\n",
            "    SM Frequency            cycle/usecond       554.94\n",
            "    Elapsed Cycles                  cycle        3,055\n",
            "    Memory Throughput                   %         0.85\n",
            "    DRAM Throughput                     %         0.85\n",
            "    Duration                      usecond         5.50\n",
            "    L1/TEX Cache Throughput             %        23.74\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.17\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.66\n",
            "    Achieved Active Warps Per SM           warp        30.93\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.92\n",
            "    SM Frequency            cycle/usecond       576.05\n",
            "    Elapsed Cycles                  cycle        7,005\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.16\n",
            "    L1/TEX Cache Throughput             %        54.49\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       147.72\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.17\n",
            "    Achieved Active Warps Per SM           warp        29.81\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.84\n",
            "    SM Frequency            cycle/usecond       563.59\n",
            "    Elapsed Cycles                  cycle        3,103\n",
            "    Memory Throughput                   %         0.85\n",
            "    DRAM Throughput                     %         0.85\n",
            "    Duration                      usecond         5.50\n",
            "    L1/TEX Cache Throughput             %        23.21\n",
            "    L2 Cache Throughput                 %         0.56\n",
            "    SM Active Cycles                cycle        48.25\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.76\n",
            "    Achieved Active Warps Per SM           warp        30.96\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.89\n",
            "    SM Frequency            cycle/usecond       571.42\n",
            "    Elapsed Cycles                  cycle        6,967\n",
            "    Memory Throughput                   %         1.16\n",
            "    DRAM Throughput                     %         0.27\n",
            "    Duration                      usecond        12.19\n",
            "    L1/TEX Cache Throughput             %        53.84\n",
            "    L2 Cache Throughput                 %         0.28\n",
            "    SM Active Cycles                cycle       149.53\n",
            "    Compute (SM) Throughput             %         1.16\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.21\n",
            "    Achieved Active Warps Per SM           warp        29.83\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.84\n",
            "    SM Frequency            cycle/usecond       562.21\n",
            "    Elapsed Cycles                  cycle        3,077\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.21\n",
            "    L2 Cache Throughput                 %         0.56\n",
            "    SM Active Cycles                cycle        48.25\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.75\n",
            "    Achieved Active Warps Per SM           warp        30.96\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.97\n",
            "    SM Frequency            cycle/usecond       581.02\n",
            "    Elapsed Cycles                  cycle        7,066\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.16\n",
            "    L1/TEX Cache Throughput             %        54.43\n",
            "    L2 Cache Throughput                 %         0.29\n",
            "    SM Active Cycles                cycle       147.90\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.39\n",
            "    Achieved Active Warps Per SM           warp        29.89\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.80\n",
            "    SM Frequency            cycle/usecond       561.62\n",
            "    Elapsed Cycles                  cycle        3,056\n",
            "    Memory Throughput                   %         0.89\n",
            "    DRAM Throughput                     %         0.89\n",
            "    Duration                      usecond         5.44\n",
            "    L1/TEX Cache Throughput             %        23.55\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.55\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.71\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.94\n",
            "    SM Frequency            cycle/usecond       577.02\n",
            "    Elapsed Cycles                  cycle        7,017\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.16\n",
            "    L1/TEX Cache Throughput             %        54.78\n",
            "    L2 Cache Throughput                 %         0.28\n",
            "    SM Active Cycles                cycle       146.95\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.06\n",
            "    Achieved Active Warps Per SM           warp        29.78\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.80\n",
            "    SM Frequency            cycle/usecond       561.43\n",
            "    Elapsed Cycles                  cycle        3,056\n",
            "    Memory Throughput                   %         0.90\n",
            "    DRAM Throughput                     %         0.90\n",
            "    Duration                      usecond         5.44\n",
            "    L1/TEX Cache Throughput             %        23.70\n",
            "    L2 Cache Throughput                 %         0.60\n",
            "    SM Active Cycles                cycle        47.25\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.69\n",
            "    Achieved Active Warps Per SM           warp        30.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.91\n",
            "    SM Frequency            cycle/usecond       574.40\n",
            "    Elapsed Cycles                  cycle        7,040\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.26\n",
            "    L1/TEX Cache Throughput             %        54.77\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       146.97\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.08\n",
            "    Achieved Active Warps Per SM           warp        29.79\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.85\n",
            "    SM Frequency            cycle/usecond       563.94\n",
            "    Elapsed Cycles                  cycle        3,141\n",
            "    Memory Throughput                   %         0.82\n",
            "    DRAM Throughput                     %         0.82\n",
            "    Duration                      usecond         5.57\n",
            "    L1/TEX Cache Throughput             %        23.42\n",
            "    L2 Cache Throughput                 %         0.56\n",
            "    SM Active Cycles                cycle        47.83\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.72\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.87\n",
            "    SM Frequency            cycle/usecond       571.06\n",
            "    Elapsed Cycles                  cycle        6,963\n",
            "    Memory Throughput                   %         1.16\n",
            "    DRAM Throughput                     %         0.27\n",
            "    Duration                      usecond        12.19\n",
            "    L1/TEX Cache Throughput             %        53.31\n",
            "    L2 Cache Throughput                 %         0.31\n",
            "    SM Active Cycles                cycle          151\n",
            "    Compute (SM) Throughput             %         1.16\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.30\n",
            "    Achieved Active Warps Per SM           warp        29.86\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.85\n",
            "    SM Frequency            cycle/usecond       568.12\n",
            "    Elapsed Cycles                  cycle        3,091\n",
            "    Memory Throughput                   %         0.86\n",
            "    DRAM Throughput                     %         0.86\n",
            "    Duration                      usecond         5.44\n",
            "    L1/TEX Cache Throughput             %        23.44\n",
            "    L2 Cache Throughput                 %         0.58\n",
            "    SM Active Cycles                cycle        47.77\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.61\n",
            "    Achieved Active Warps Per SM           warp        30.92\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.93\n",
            "    SM Frequency            cycle/usecond       576.05\n",
            "    Elapsed Cycles                  cycle        7,005\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.16\n",
            "    L1/TEX Cache Throughput             %        54.80\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       146.90\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.04\n",
            "    Achieved Active Warps Per SM           warp        29.77\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.79\n",
            "    SM Frequency            cycle/usecond       557.12\n",
            "    Elapsed Cycles                  cycle        3,067\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.50\n",
            "    L1/TEX Cache Throughput             %        23.43\n",
            "    L2 Cache Throughput                 %         0.56\n",
            "    SM Active Cycles                cycle        47.80\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.72\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.94\n",
            "    SM Frequency            cycle/usecond       577.51\n",
            "    Elapsed Cycles                  cycle        7,042\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.19\n",
            "    L1/TEX Cache Throughput             %        55.42\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       145.25\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        92.98\n",
            "    Achieved Active Warps Per SM           warp        29.75\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.77\n",
            "    SM Frequency            cycle/usecond       556.29\n",
            "    Elapsed Cycles                  cycle        3,062\n",
            "    Memory Throughput                   %         0.90\n",
            "    DRAM Throughput                     %         0.90\n",
            "    Duration                      usecond         5.50\n",
            "    L1/TEX Cache Throughput             %        23.41\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.85\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.72\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.89\n",
            "    SM Frequency            cycle/usecond       571.73\n",
            "    Elapsed Cycles                  cycle        6,989\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.22\n",
            "    L1/TEX Cache Throughput             %        54.67\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       147.25\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.14\n",
            "    Achieved Active Warps Per SM           warp        29.81\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.75\n",
            "    SM Frequency            cycle/usecond       558.49\n",
            "    Elapsed Cycles                  cycle        3,039\n",
            "    Memory Throughput                   %         0.90\n",
            "    DRAM Throughput                     %         0.90\n",
            "    Duration                      usecond         5.44\n",
            "    L1/TEX Cache Throughput             %        23.70\n",
            "    L2 Cache Throughput                 %         0.60\n",
            "    SM Active Cycles                cycle        47.25\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.66\n",
            "    Achieved Active Warps Per SM           warp        30.93\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.91\n",
            "    SM Frequency            cycle/usecond       575.64\n",
            "    Elapsed Cycles                  cycle        7,019\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.19\n",
            "    L1/TEX Cache Throughput             %        54.73\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       147.07\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.05\n",
            "    Achieved Active Warps Per SM           warp        29.78\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.80\n",
            "    SM Frequency            cycle/usecond       563.24\n",
            "    Elapsed Cycles                  cycle        3,065\n",
            "    Memory Throughput                   %         0.89\n",
            "    DRAM Throughput                     %         0.89\n",
            "    Duration                      usecond         5.44\n",
            "    L1/TEX Cache Throughput             %        23.02\n",
            "    L2 Cache Throughput                 %         0.56\n",
            "    SM Active Cycles                cycle        48.65\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.73\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.88\n",
            "    SM Frequency            cycle/usecond       571.78\n",
            "    Elapsed Cycles                  cycle        7,027\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.29\n",
            "    L1/TEX Cache Throughput             %        55.05\n",
            "    L2 Cache Throughput                 %         0.31\n",
            "    SM Active Cycles                cycle       146.22\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.03\n",
            "    Achieved Active Warps Per SM           warp        29.77\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.66\n",
            "    SM Frequency            cycle/usecond       544.10\n",
            "    Elapsed Cycles                  cycle        3,083\n",
            "    Memory Throughput                   %         0.86\n",
            "    DRAM Throughput                     %         0.86\n",
            "    Duration                      usecond         5.66\n",
            "    L1/TEX Cache Throughput             %        23.53\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.60\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.70\n",
            "    Achieved Active Warps Per SM           warp        30.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.85\n",
            "    SM Frequency            cycle/usecond       567.32\n",
            "    Elapsed Cycles                  cycle        7,044\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.42\n",
            "    L1/TEX Cache Throughput             %        54.89\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       146.65\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.08\n",
            "    Achieved Active Warps Per SM           warp        29.79\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.77\n",
            "    SM Frequency            cycle/usecond       558.77\n",
            "    Elapsed Cycles                  cycle        3,058\n",
            "    Memory Throughput                   %         0.88\n",
            "    DRAM Throughput                     %         0.88\n",
            "    Duration                      usecond         5.47\n",
            "    L1/TEX Cache Throughput             %        23.47\n",
            "    L2 Cache Throughput                 %         0.60\n",
            "    SM Active Cycles                cycle        47.73\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.71\n",
            "    Achieved Active Warps Per SM           warp        30.95\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.82\n",
            "    SM Frequency            cycle/usecond       566.09\n",
            "    Elapsed Cycles                  cycle        6,975\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.27\n",
            "    Duration                      usecond        12.32\n",
            "    L1/TEX Cache Throughput             %        54.64\n",
            "    L2 Cache Throughput                 %         0.31\n",
            "    SM Active Cycles                cycle       147.32\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.00\n",
            "    Achieved Active Warps Per SM           warp        29.76\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.76\n",
            "    SM Frequency            cycle/usecond       555.17\n",
            "    Elapsed Cycles                  cycle        3,074\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.54\n",
            "    L1/TEX Cache Throughput             %        23.60\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.45\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.70\n",
            "    Achieved Active Warps Per SM           warp        30.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.94\n",
            "    SM Frequency            cycle/usecond       579.56\n",
            "    Elapsed Cycles                  cycle        7,085\n",
            "    Memory Throughput                   %         1.14\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.22\n",
            "    L1/TEX Cache Throughput             %        54.91\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       146.60\n",
            "    Compute (SM) Throughput             %         1.14\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.03\n",
            "    Achieved Active Warps Per SM           warp        29.77\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.83\n",
            "    SM Frequency            cycle/usecond       565.16\n",
            "    Elapsed Cycles                  cycle        3,057\n",
            "    Memory Throughput                   %         0.87\n",
            "    DRAM Throughput                     %         0.87\n",
            "    Duration                      usecond         5.41\n",
            "    L1/TEX Cache Throughput             %        22.86\n",
            "    L2 Cache Throughput                 %         0.61\n",
            "    SM Active Cycles                cycle           49\n",
            "    Compute (SM) Throughput             %         0.37\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.81\n",
            "    Achieved Active Warps Per SM           warp        30.98\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.88\n",
            "    SM Frequency            cycle/usecond       571.84\n",
            "    Elapsed Cycles                  cycle        6,991\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.22\n",
            "    L1/TEX Cache Throughput             %        54.31\n",
            "    L2 Cache Throughput                 %         0.30\n",
            "    SM Active Cycles                cycle       148.22\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.12\n",
            "    Achieved Active Warps Per SM           warp        29.80\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  cudaNodeRelax(int, int *, int *, int *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.74\n",
            "    SM Frequency            cycle/usecond       557.85\n",
            "    Elapsed Cycles                  cycle        3,071\n",
            "    Memory Throughput                   %         0.88\n",
            "    DRAM Throughput                     %         0.88\n",
            "    Duration                      usecond         5.50\n",
            "    L1/TEX Cache Throughput             %        23.52\n",
            "    L2 Cache Throughput                 %         0.57\n",
            "    SM Active Cycles                cycle        47.62\n",
            "    Compute (SM) Throughput             %         0.36\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            4.10\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            8\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.60\n",
            "    Achieved Active Warps Per SM           warp        30.91\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  findMinReductionCUDA(int *, int *, int, int *, int *) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.91\n",
            "    SM Frequency            cycle/usecond       575.08\n",
            "    Elapsed Cycles                  cycle        7,012\n",
            "    Memory Throughput                   %         1.15\n",
            "    DRAM Throughput                     %         0.26\n",
            "    Duration                      usecond        12.19\n",
            "    L1/TEX Cache Throughput             %        54.90\n",
            "    L2 Cache Throughput                 %         0.27\n",
            "    SM Active Cycles                cycle       146.62\n",
            "    Compute (SM) Throughput             %         1.15\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block      Kbyte/block            8.19\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread           1,024\n",
            "    Waves Per SM                                                0.03\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block            4\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        93.42\n",
            "    Achieved Active Warps Per SM           warp        29.89\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./dijkstra_serialMerge_3kern.o"
      ],
      "metadata": {
        "id": "BbLCbU44yQMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile debug.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <cfloat> // Per FLT_MAX\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <chrono>\n",
        "\n",
        "// Funzione per eseguire Dijkstra in modo seriale sulla CPU\n",
        "void dijkstraSerial(const float *graph, int n, int start, float *distances) {\n",
        "    // Array per tenere traccia dei nodi visitati\n",
        "    std::vector<int> visited(n, 0);\n",
        "\n",
        "    // Inizializza le distanze\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        distances[i] = (i == start) ? 0.0f : FLT_MAX;\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        float minDist = FLT_MAX;\n",
        "        int currentNode = -1;\n",
        "\n",
        "        // Trova il nodo con la distanza minima non ancora visitato\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            if (!visited[j] && distances[j] < minDist) {\n",
        "                minDist = distances[j];\n",
        "                currentNode = j;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if (currentNode == -1) break; // Nessun altro nodo raggiungibile\n",
        "\n",
        "        // Marca il nodo corrente come visitato\n",
        "        visited[currentNode] = 1;\n",
        "\n",
        "        // Aggiorna le distanze per i nodi adiacenti\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            float weight = graph[currentNode * n + j]; // Peso dell'arco currentNode -> j\n",
        "            if (weight > 0.0f && weight < FLT_MAX) {  // Esiste un arco valido\n",
        "                // Verifica per evitare overflow: dist[currentNode] + weight\n",
        "                if (distances[currentNode] != FLT_MAX &&\n",
        "                    distances[currentNode] + weight < distances[j]) {\n",
        "                    distances[j] = distances[currentNode] + weight;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "// Kernel CUDA per aggiornare le distanze\n",
        "__global__ void cudaNodeRelax(int nodo, float *graph, float *distances, int *visited, int n) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Carica la riga del nodo corrente nella memoria condivisa\n",
        "    extern __shared__ float sharedRow[];\n",
        "    if (tid < n) {\n",
        "        sharedRow[tid] = graph[nodo * n + tid];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    if (tid < n && visited[tid] == 0) { // Assicurati che il nodo non sia stato visitato\n",
        "        float weight = sharedRow[tid]; // Peso dell'arco (nodo -> tid)\n",
        "        if (weight < FLT_MAX && weight > 0.0f) { // Arco esistente e diverso da se stesso\n",
        "            atomicMin((int *)&distances[tid], __float_as_int(distances[nodo] + weight)); // Aggiorna la distanza con atomicMin\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void findMinReductionCUDA(float *distanze, int *visitato, int n, int *minNodo, float *minDistanza) {\n",
        "    // Memoria condivisa dinamica\n",
        "    extern __shared__ float sharedMem[];\n",
        "\n",
        "    // Dividi la memoria condivisa in due array distinti\n",
        "    float *sharedDistanze = sharedMem;                        // Primo array: memorizza le distanze\n",
        "    int *sharedNodi = (int *)&sharedDistanze[n];              // Secondo array: memorizza gli indici dei nodi\n",
        "\n",
        "    int tid = threadIdx.x;                   // Indice del thread locale al blocco\n",
        "    int gid = blockIdx.x * blockDim.x + tid; // Indice globale del thread\n",
        "\n",
        "    // Inizializzazione della memoria condivisa\n",
        "    if (gid < n && tid < blockDim.x) {\n",
        "        sharedDistanze[tid] = (visitato[gid] == 0) ? distanze[gid] : FLT_MAX;\n",
        "        sharedNodi[tid] = gid;\n",
        "    } else {\n",
        "        sharedDistanze[tid] = FLT_MAX;\n",
        "        sharedNodi[tid] = -1;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Riduzione parallela per trovare il minimo\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        if (tid + stride < blockDim.x && sharedDistanze[tid + stride] < sharedDistanze[tid]) {\n",
        "            sharedDistanze[tid] = sharedDistanze[tid + stride];\n",
        "            sharedNodi[tid] = sharedNodi[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Il thread 0 del blocco salva il risultato parziale in modo atomico\n",
        "    if (tid == 0) {\n",
        "        atomicExch(minNodo, sharedNodi[0]);\n",
        "        atomicExch((int *)minDistanza, __float_as_int(sharedDistanze[0]));\n",
        "        atomicExch(&visitato[sharedNodi[0]], 1); // Segna come visitato\n",
        "    }\n",
        "}\n",
        "__global__ void initialValueKernel(float *distances, int *visited, int start, int n) {\n",
        "    int id = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (id < n) {\n",
        "        distances[id] = (id == start) ? 0.0f : FLT_MAX;\n",
        "        visited[id] = 0;\n",
        "    }\n",
        "}\n",
        "\n",
        "void dijkstraCUDA(float *graph, int n, int start, float *distances) {\n",
        "    // Allocazione memoria su GPU\n",
        "    float *d_graph, *d_minDistanza, *d_distances;\n",
        "    int *d_minNodo, *d_visited;\n",
        "\n",
        "    // Allocazione memoria sulla GPU\n",
        "    cudaMalloc((void **)&d_graph, n * n * sizeof(float));\n",
        "    cudaMalloc((void **)&d_distances, n * sizeof(float));\n",
        "    cudaMalloc((void **)&d_visited, n * sizeof(int));\n",
        "    cudaMalloc((void **)&d_minNodo, sizeof(int));\n",
        "    cudaMalloc((void **)&d_minDistanza, sizeof(float));\n",
        "\n",
        "    int blockSize = n;\n",
        "    int gridDim = (n + blockSize - 1) / blockSize;\n",
        "\n",
        "    // Inizializza distanze e visitati\n",
        "    initialValueKernel<<<gridDim, blockSize>>>(d_distances, d_visited, start, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copia il grafo sulla GPU\n",
        "    cudaMemcpy(d_graph, graph, n * n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        // Resetta `minDistanza` al massimo valore possibile\n",
        "        float h_minDistanza = FLT_MAX;\n",
        "        cudaMemcpy(d_minDistanza, &h_minDistanza, sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Trova il nodo con la distanza minima non visitato\n",
        "        blockSize = n;\n",
        "        gridDim = (n + blockSize - 1) / blockSize;\n",
        "        findMinReductionCUDA<<<gridDim, blockSize, blockSize * (sizeof(float) + sizeof(int))>>>(\n",
        "            d_distances, d_visited, n, d_minNodo, d_minDistanza);\n",
        "\n",
        "        if (cudaDeviceSynchronize() != cudaSuccess) {\n",
        "            std::cerr << \"Errore durante l'esecuzione del kernel.\" << std::endl;\n",
        "            return;\n",
        "        }\n",
        "\n",
        "        // Recupera il nodo con distanza minima dalla GPU\n",
        "        int h_minNodo;\n",
        "        cudaMemcpy(&h_minNodo, d_minNodo, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "        cudaMemcpy(&h_minDistanza, d_minDistanza, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Se non ci sono piÃ¹ nodi raggiungibili, esci\n",
        "        if (h_minNodo == -1 || h_minDistanza == FLT_MAX) {\n",
        "            break;\n",
        "        }\n",
        "\n",
        "        // Aggiorna le distanze per il nodo corrente\n",
        "        cudaNodeRelax<<<gridDim, blockSize, blockSize * sizeof(float)>>>(h_minNodo, d_graph, d_distances, d_visited, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    // Copia i risultati dalla GPU alla CPU\n",
        "    cudaMemcpy(distances, d_distances, n * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Libera memoria GPU\n",
        "    cudaFree(d_graph);\n",
        "    cudaFree(d_distances);\n",
        "    cudaFree(d_visited);\n",
        "    cudaFree(d_minNodo);\n",
        "    cudaFree(d_minDistanza);\n",
        "}\n",
        "__global__ void generateGraphKernel(float *graph, int n, float minWeight, float maxWeight, unsigned int seed) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Stato del generatore casuale\n",
        "    curandState state;\n",
        "\n",
        "    // Grid-stride loop per coprire tutta la matrice\n",
        "    for (int i = idx; i < n * n; i += blockDim.x * gridDim.x) {\n",
        "        int row = i / n; // Calcola la riga\n",
        "        int col = i % n; // Calcola la colonna\n",
        "\n",
        "        // Inizializza lo stato casuale\n",
        "        curand_init(seed + i, 0, 0, &state);\n",
        "\n",
        "        // Calcola il peso per la triangolare superiore\n",
        "        if (row < col) {\n",
        "            float randomProb = curand_uniform(&state);\n",
        "            float weight = (randomProb < 0.4f)\n",
        "                           ? curand_uniform(&state) * (maxWeight - minWeight) + minWeight\n",
        "                           : FLT_MAX;\n",
        "\n",
        "            // Scrive nella memoria globale\n",
        "            graph[row * n + col] = weight;\n",
        "            graph[col * n + row] = weight; // Simmetria\n",
        "        } else if (row == col) {\n",
        "            // Imposta la diagonale a 0\n",
        "            graph[row * n + col] = 0.0f;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void generateGraphCUDA(float *graph, int n, float minWeight, float maxWeight) {\n",
        "    float *d_graph;\n",
        "    size_t size = n * n * sizeof(float);\n",
        "\n",
        "    // Allocazione della memoria sulla GPU\n",
        "    if (cudaMalloc(&d_graph, size) != cudaSuccess) {\n",
        "        std::cerr << \"Errore nell'allocazione della memoria sulla GPU.\" << std::endl;\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    if (cudaMemset(d_graph, 0, size) != cudaSuccess) {\n",
        "        std::cerr << \"Errore durante l'inizializzazione della memoria sulla GPU.\" << std::endl;\n",
        "        cudaFree(d_graph);\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Configura blocchi e griglia\n",
        "    int threadsPerBlock = 1024;\n",
        "    int blocksPerGrid = (n * n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    // Lancia il kernel\n",
        "    generateGraphKernel<<<blocksPerGrid, threadsPerBlock>>>(d_graph, n, minWeight, maxWeight, time(NULL));\n",
        "\n",
        "    if (cudaDeviceSynchronize() != cudaSuccess) {\n",
        "        std::cerr << \"Errore durante l'esecuzione del kernel.\" << std::endl;\n",
        "        cudaFree(d_graph);\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Copia i dati dalla GPU alla CPU\n",
        "    if (cudaMemcpy(graph, d_graph, size, cudaMemcpyDeviceToHost) != cudaSuccess) {\n",
        "        std::cerr << \"Errore nella copia dei dati dalla GPU alla CPU.\" << std::endl;\n",
        "        cudaFree(d_graph);\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Libera memoria GPU\n",
        "    cudaFree(d_graph);\n",
        "}\n",
        "\n",
        "void generateGraphSerial(float *graph, int n, float minWeight, float maxWeight) {\n",
        "    // Inizializza il generatore di numeri casuali\n",
        "    std::srand(std::time(0));\n",
        "\n",
        "    for (int row = 0; row < n; ++row) {\n",
        "        for (int col = 0; col < n; ++col) {\n",
        "            if (row == col) {\n",
        "                // Imposta la diagonale principale a 0\n",
        "                graph[row * n + col] = 0.0f;\n",
        "            } else if (row < col) {\n",
        "                // Genera un arco casuale per la triangolare superiore\n",
        "                int randomValue = std::rand() % 100;\n",
        "                if (randomValue < 40) { // 40% probabilitÃ  di avere un arco\n",
        "                    float weight = static_cast<float>(std::rand()) / RAND_MAX * (maxWeight - minWeight) + minWeight;\n",
        "                    graph[row * n + col] = weight;\n",
        "                    graph[col * n + row] = weight; // Simmetria\n",
        "                } else {\n",
        "                    graph[row * n + col] = FLT_MAX; // Nessun arco\n",
        "                    graph[col * n + row] = FLT_MAX; // Nessun arco\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 4096; // Numero di nodi\n",
        "    int start = 0; // Nodo iniziale\n",
        "    float minWeight = 1.0f;\n",
        "    float maxWeight = 2000.0f;\n",
        "\n",
        "    std::vector<float> graph(n * n);\n",
        "    std::vector<float> graphSerial(n * n);\n",
        "\n",
        "     // Generazione del grafo Seriale\n",
        "    auto startTimeS = std::chrono::high_resolution_clock::now();\n",
        "    generateGraphSerial(graphSerial.data(), n, minWeight, maxWeight);\n",
        "    auto endTimeS = std::chrono::high_resolution_clock::now();\n",
        "    long serialGraphTime = std::chrono::duration_cast<std::chrono::microseconds>(endTimeS - startTimeS).count();\n",
        "\n",
        "\n",
        "    // Generazione del grafo con CUDA\n",
        "    auto startTime = std::chrono::high_resolution_clock::now();\n",
        "    generateGraphCUDA(graph.data(), n, minWeight, maxWeight);\n",
        "    auto endTime = std::chrono::high_resolution_clock::now();\n",
        "    long cudaGraphTime = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();\n",
        "\n",
        "    // Calcolo delle distanze con Dijkstra Seriale\n",
        "    std::vector<float> serialDistances(n);\n",
        "    startTime = std::chrono::high_resolution_clock::now();\n",
        "    dijkstraSerial(graph.data(), n, start, serialDistances.data());\n",
        "    endTime = std::chrono::high_resolution_clock::now();\n",
        "    long serialTime = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();\n",
        "\n",
        "    // Calcolo delle distanze con Dijkstra CUDA\n",
        "    std::vector<float> cudaDistances(n);\n",
        "    startTime = std::chrono::high_resolution_clock::now();\n",
        "    dijkstraCUDA(graph.data(), n, start, cudaDistances.data());\n",
        "    endTime = std::chrono::high_resolution_clock::now();\n",
        "    long cudaTime = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();\n",
        "\n",
        "  /*\n",
        "    // Stampa del grafo\n",
        "    std::cout << \"=== Matrice del grafo ===\\n\";\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            if (i == j) {\n",
        "                std::cout << \"X \"; // Diagonale principale\n",
        "            } else if (graph[i * n + j] == FLT_MAX) {\n",
        "                std::cout << \"_ \"; // Nessun collegamento\n",
        "            } else {\n",
        "                std::cout << graph[i * n + j] << \" \"; // Peso dell'arco\n",
        "            }\n",
        "        }\n",
        "        std::cout << \"\\n\";\n",
        "    }\n",
        "\n",
        "    // Confronto distanze Seriali vs CUDA\n",
        "    std::cout << \"\\n=== Distanze (Seriale vs CUDA) ===\\n\";\n",
        "    std::cout << \"Nodo\\tSeriale\\tCUDA\\n\";\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        std::cout << i << \"\\t\";\n",
        "        if (serialDistances[i] == FLT_MAX)\n",
        "            std::cout << \"â\\t\";\n",
        "        else\n",
        "            std::cout << serialDistances[i] << \"\\t\";\n",
        "\n",
        "        if (cudaDistances[i] == FLT_MAX)\n",
        "            std::cout << \"â\\n\";\n",
        "        else\n",
        "            std::cout << cudaDistances[i] << \"\\n\";\n",
        "    }\n",
        "  */\n",
        "\n",
        "    // Stampa dei tempi di generazione e di esecuzione\n",
        "    std::cout << \"\\n=== Tempi di generazione ===\\n\";\n",
        "    std::cout << \"Tempo di generazione Seriale: \" << serialGraphTime << \" Âµs\\n\";\n",
        "    std::cout << \"Tempo di generazione CUDA: \" << cudaGraphTime << \" Âµs\\n\";\n",
        "\n",
        "    std::cout << \"\\n=== Tempi di esecuzione Dijkstra ===\\n\";\n",
        "    std::cout << \"Tempo esecuzione Seriale: \" << serialTime << \" Âµs\\n\";\n",
        "    std::cout << \"Tempo esecuzione CUDA: \" << cudaTime << \" Âµs\\n\";\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Oskv-60LPPg",
        "outputId": "d9e28014-addc-44cd-9d70-61f27a9646df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting debug.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -G -g debug.cu -o debug.o\n",
        "!./debug.o"
      ],
      "metadata": {
        "id": "3iBCpCW5LW85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --set full -o test_rep3 ./debug.o"
      ],
      "metadata": {
        "id": "A_OOns8Qz8ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile debug_double.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <cfloat> // Per DBL_MAX\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <chrono>\n",
        "\n",
        "// Funzione per eseguire Dijkstra in modo seriale sulla CPU\n",
        "void dijkstraSerial(const double *graph, int n, int start, double *distances) {\n",
        "    // Array per tenere traccia dei nodi visitati\n",
        "    std::vector<int> visited(n, 0);\n",
        "\n",
        "    // Inizializza le distanze\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        distances[i] = (i == start) ? 0.0 : DBL_MAX;\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        double minDist = DBL_MAX;\n",
        "        int currentNode = -1;\n",
        "\n",
        "        // Trova il nodo con la distanza minima non ancora visitato\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            if (!visited[j] && distances[j] < minDist) {\n",
        "                minDist = distances[j];\n",
        "                currentNode = j;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if (currentNode == -1) break; // Nessun altro nodo raggiungibile\n",
        "\n",
        "        // Marca il nodo corrente come visitato\n",
        "        visited[currentNode] = 1;\n",
        "\n",
        "        // Aggiorna le distanze per i nodi adiacenti\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            double weight = graph[currentNode * n + j]; // Peso dell'arco currentNode -> j\n",
        "            if (weight > 0.0 && weight < DBL_MAX) {  // Esiste un arco valido\n",
        "                // Verifica per evitare overflow: dist[currentNode] + weight\n",
        "                if (distances[currentNode] != DBL_MAX &&\n",
        "                    distances[currentNode] + weight < distances[j]) {\n",
        "                    distances[j] = distances[currentNode] + weight;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel CUDA per aggiornare le distanze\n",
        "__global__ void cudaNodeRelax(int nodo, double *graph, double *distances, int *visited, int n) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Carica la riga del nodo corrente nella memoria condivisa\n",
        "    extern __shared__ double sharedRow[];\n",
        "    if (tid < n) {\n",
        "        sharedRow[tid] = graph[nodo * n + tid];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    if (tid < n && visited[tid] == 0) { // Assicurati che il nodo non sia stato visitato\n",
        "        double weight = sharedRow[tid]; // Peso dell'arco (nodo -> tid)\n",
        "        if (weight < DBL_MAX && weight > 0.0) { // Arco esistente e diverso da se stesso\n",
        "            atomicMin((long long *)&distances[tid], __double_as_longlong(distances[nodo] + weight)); // Aggiorna la distanza con atomicMin\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void findMinReductionCUDA(double *distanze, int *visitato, int n, int *minNodo, double *minDistanza) {\n",
        "    // Memoria condivisa dinamica\n",
        "    extern __shared__ double sharedMem[];\n",
        "\n",
        "    // Dividi la memoria condivisa in due array distinti\n",
        "    double *sharedDistanze = sharedMem;                        // Primo array: memorizza le distanze\n",
        "    int *sharedNodi = (int *)&sharedDistanze[n];               // Secondo array: memorizza gli indici dei nodi\n",
        "\n",
        "    int tid = threadIdx.x;                   // Indice del thread locale al blocco\n",
        "    int gid = blockIdx.x * blockDim.x + tid; // Indice globale del thread\n",
        "\n",
        "    // Inizializzazione della memoria condivisa\n",
        "    if (gid < n && tid < blockDim.x) {\n",
        "        sharedDistanze[tid] = (visitato[gid] == 0) ? distanze[gid] : DBL_MAX;\n",
        "        sharedNodi[tid] = gid;\n",
        "    } else {\n",
        "        sharedDistanze[tid] = DBL_MAX;\n",
        "        sharedNodi[tid] = -1;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Riduzione parallela per trovare il minimo\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        if (tid + stride < blockDim.x && sharedDistanze[tid + stride] < sharedDistanze[tid]) {\n",
        "            sharedDistanze[tid] = sharedDistanze[tid + stride];\n",
        "            sharedNodi[tid] = sharedNodi[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Il thread 0 del blocco salva il risultato parziale in modo atomico\n",
        "    if (tid == 0) {\n",
        "        atomicExch(minNodo, sharedNodi[0]);\n",
        "        atomicExch((long long *)minDistanza, __double_as_longlong(sharedDistanze[0]));\n",
        "        atomicExch(&visitato[sharedNodi[0]], 1); // Segna come visitato\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void initialValueKernel(double *distances, int *visited, int start, int n) {\n",
        "    int id = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (id < n) {\n",
        "        distances[id] = (id == start) ? 0.0 : DBL_MAX;\n",
        "        visited[id] = 0;\n",
        "    }\n",
        "}\n",
        "void dijkstraCUDA(double *graph, int n, int start, double *distances) {\n",
        "    // Allocazione memoria su GPU\n",
        "    double *d_graph, *d_minDistanza, *d_distances;\n",
        "    int *d_minNodo, *d_visited;\n",
        "\n",
        "    // Allocazione memoria sulla GPU\n",
        "    cudaMalloc((void **)&d_graph, n * n * sizeof(double));\n",
        "    cudaMalloc((void **)&d_distances, n * sizeof(double));\n",
        "    cudaMalloc((void **)&d_visited, n * sizeof(int));\n",
        "    cudaMalloc((void **)&d_minNodo, sizeof(int));\n",
        "    cudaMalloc((void **)&d_minDistanza, sizeof(double));\n",
        "\n",
        "    int blockSize = n;\n",
        "    int gridDim = (n + blockSize - 1) / blockSize;\n",
        "\n",
        "    // Inizializza distanze e visitati\n",
        "    initialValueKernel<<<gridDim, blockSize>>>(d_distances, d_visited, start, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copia il grafo sulla GPU\n",
        "    cudaMemcpy(d_graph, graph, n * n * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        // Resetta `minDistanza` al massimo valore possibile\n",
        "        double h_minDistanza = DBL_MAX;\n",
        "        cudaMemcpy(d_minDistanza, &h_minDistanza, sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Trova il nodo con la distanza minima non visitato\n",
        "        blockSize = n;\n",
        "        gridDim = (n + blockSize - 1) / blockSize;\n",
        "        findMinReductionCUDA<<<gridDim, blockSize, blockSize * (sizeof(double) + sizeof(int))>>>(\n",
        "            d_distances, d_visited, n, d_minNodo, d_minDistanza);\n",
        "\n",
        "        if (cudaDeviceSynchronize() != cudaSuccess) {\n",
        "            std::cerr << \"Errore durante l'esecuzione del kernel.\" << std::endl;\n",
        "            return;\n",
        "        }\n",
        "\n",
        "        // Recupera il nodo con distanza minima dalla GPU\n",
        "        int h_minNodo;\n",
        "        cudaMemcpy(&h_minNodo, d_minNodo, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "        cudaMemcpy(&h_minDistanza, d_minDistanza, sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Se non ci sono piÃ¹ nodi raggiungibili, esci\n",
        "        if (h_minNodo == -1 || h_minDistanza == DBL_MAX) {\n",
        "            break;\n",
        "        }\n",
        "\n",
        "        // Aggiorna le distanze per il nodo corrente\n",
        "        cudaNodeRelax<<<gridDim, blockSize, blockSize * sizeof(double)>>>(h_minNodo, d_graph, d_distances, d_visited, n);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    // Copia i risultati dalla GPU alla CPU\n",
        "    cudaMemcpy(distances, d_distances, n * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Libera memoria GPU\n",
        "    cudaFree(d_graph);\n",
        "    cudaFree(d_distances);\n",
        "    cudaFree(d_visited);\n",
        "    cudaFree(d_minNodo);\n",
        "    cudaFree(d_minDistanza);\n",
        "}\n",
        "\n",
        "__global__ void generateGraphKernel(double *graph, int n, double minWeight, double maxWeight, unsigned int seed) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Stato del generatore casuale\n",
        "    curandState state;\n",
        "\n",
        "    // Grid-stride loop per coprire tutta la matrice\n",
        "    for (int i = idx; i < n * n; i += blockDim.x * gridDim.x) {\n",
        "        int row = i / n; // Calcola la riga\n",
        "        int col = i % n; // Calcola la colonna\n",
        "\n",
        "        // Inizializza lo stato casuale\n",
        "        curand_init(seed + i, 0, 0, &state);\n",
        "\n",
        "        // Calcola il peso per la triangolare superiore\n",
        "        if (row < col) {\n",
        "            double randomProb = curand_uniform_double(&state);\n",
        "            double weight = (randomProb < 0.4)\n",
        "                           ? curand_uniform_double(&state) * (maxWeight - minWeight) + minWeight\n",
        "                           : DBL_MAX;\n",
        "\n",
        "            // Scrive nella memoria globale\n",
        "            graph[row * n + col] = weight;\n",
        "            graph[col * n + row] = weight; // Simmetria\n",
        "        } else if (row == col) {\n",
        "            // Imposta la diagonale a 0\n",
        "            graph[row * n + col] = 0.0;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "void generateGraphCUDA(double *graph, int n, double minWeight, double maxWeight) {\n",
        "    double *d_graph;\n",
        "    size_t size = n * n * sizeof(double);\n",
        "\n",
        "    // Allocazione della memoria sulla GPU\n",
        "    if (cudaMalloc(&d_graph, size) != cudaSuccess) {\n",
        "        std::cerr << \"Errore nell'allocazione della memoria sulla GPU.\" << std::endl;\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    if (cudaMemset(d_graph, 0, size) != cudaSuccess) {\n",
        "        std::cerr << \"Errore durante l'inizializzazione della memoria sulla GPU.\" << std::endl;\n",
        "        cudaFree(d_graph);\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Configura blocchi e griglia\n",
        "    int threadsPerBlock = 1024;\n",
        "    int blocksPerGrid = (n * n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    // Lancia il kernel\n",
        "    generateGraphKernel<<<blocksPerGrid, threadsPerBlock>>>(d_graph, n, minWeight, maxWeight, time(NULL));\n",
        "\n",
        "    if (cudaDeviceSynchronize() != cudaSuccess) {\n",
        "        std::cerr << \"Errore durante l'esecuzione del kernel.\" << std::endl;\n",
        "        cudaFree(d_graph);\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Copia i dati dalla GPU alla CPU\n",
        "    if (cudaMemcpy(graph, d_graph, size, cudaMemcpyDeviceToHost) != cudaSuccess) {\n",
        "        std::cerr << \"Errore nella copia dei dati dalla GPU alla CPU.\" << std::endl;\n",
        "        cudaFree(d_graph);\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Libera memoria GPU\n",
        "    cudaFree(d_graph);\n",
        "}\n",
        "\n",
        "void generateGraphSerial(double *graph, int n, double minWeight, double maxWeight) {\n",
        "    // Inizializza il generatore di numeri casuali\n",
        "    std::srand(std::time(0));\n",
        "\n",
        "    for (int row = 0; row < n; ++row) {\n",
        "        for (int col = 0; col < n; ++col) {\n",
        "            if (row == col) {\n",
        "                // Imposta la diagonale principale a 0\n",
        "                graph[row * n + col] = 0.0;\n",
        "            } else if (row < col) {\n",
        "                // Genera un arco casuale per la triangolare superiore\n",
        "                int randomValue = std::rand() % 100;\n",
        "                if (randomValue < 40) { // 40% probabilitÃ  di avere un arco\n",
        "                    double weight = static_cast<double>(std::rand()) / RAND_MAX * (maxWeight - minWeight) + minWeight;\n",
        "                    graph[row * n + col] = weight;\n",
        "                    graph[col * n + row] = weight; // Simmetria\n",
        "                } else {\n",
        "                    graph[row * n + col] = DBL_MAX; // Nessun arco\n",
        "                    graph[col * n + row] = DBL_MAX; // Nessun arco\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 4096; // Numero di nodi\n",
        "    int start = 0; // Nodo iniziale\n",
        "    double minWeight = 1.0;\n",
        "    double maxWeight = 2000.0;\n",
        "\n",
        "    std::vector<double> graph(n * n);\n",
        "    std::vector<double> graphSerial(n * n);\n",
        "\n",
        "    // Generazione del grafo Seriale\n",
        "    auto startTimeS = std::chrono::high_resolution_clock::now();\n",
        "    generateGraphSerial(graphSerial.data(), n, minWeight, maxWeight);\n",
        "    auto endTimeS = std::chrono::high_resolution_clock::now();\n",
        "    long serialGraphTime = std::chrono::duration_cast<std::chrono::microseconds>(endTimeS - startTimeS).count();\n",
        "\n",
        "    // Generazione del grafo con CUDA\n",
        "    auto startTime = std::chrono::high_resolution_clock::now();\n",
        "    generateGraphCUDA(graph.data(), n, minWeight, maxWeight);\n",
        "    auto endTime = std::chrono::high_resolution_clock::now();\n",
        "    long cudaGraphTime = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();\n",
        "\n",
        "    // Calcolo delle distanze con Dijkstra Seriale\n",
        "    std::vector<double> serialDistances(n);\n",
        "    startTime = std::chrono::high_resolution_clock::now();\n",
        "    dijkstraSerial(graph.data(), n, start, serialDistances.data());\n",
        "    endTime = std::chrono::high_resolution_clock::now();\n",
        "    long serialTime = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();\n",
        "\n",
        "    // Calcolo delle distanze con Dijkstra CUDA\n",
        "    std::vector<double> cudaDistances(n);\n",
        "    startTime = std::chrono::high_resolution_clock::now();\n",
        "    dijkstraCUDA(graph.data(), n, start, cudaDistances.data());\n",
        "    endTime = std::chrono::high_resolution_clock::now();\n",
        "    long cudaTime = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();\n",
        "\n",
        "    // Stampa dei tempi di generazione e di esecuzione\n",
        "    std::cout << \"\\n=== Tempi di generazione ===\\n\";\n",
        "    std::cout << \"Tempo di generazione Seriale: \" << serialGraphTime << \" Âµs\\n\";\n",
        "    std::cout << \"Tempo di generazione CUDA: \" << cudaGraphTime << \" Âµs\\n\";\n",
        "\n",
        "    std::cout << \"\\n=== Tempi di esecuzione Dijkstra ===\\n\";\n",
        "    std::cout << \"Tempo esecuzione Seriale: \" << serialTime << \" Âµs\\n\";\n",
        "    std::cout << \"Tempo esecuzione CUDA: \" << cudaTime << \" Âµs\\n\";\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "MjDR4PfBwHuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -G -g debug_double.cu -o debug_double.o\n",
        "!./debug_double.o"
      ],
      "metadata": {
        "id": "hbk9LAajwmzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --set full -o test_rep3 ./debug_double.o"
      ],
      "metadata": {
        "id": "gQwzHMnVws0o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}